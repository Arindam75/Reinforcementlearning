{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "FMNBAjTne9or",
   "metadata": {
    "id": "FMNBAjTne9or"
   },
   "source": [
    "[Hands on RL Policy Gradient](https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%204/4.3%20Policy%20Gradients%20REINFORCE.ipynb)\n",
    "\n",
    "[Policy Gradient Math](https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245)\n",
    "\n",
    "A widely used variation of REINFORCE is to subtract a baseline value from the return to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible). For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage:\n",
    "\n",
    "$$\n",
    "A(s,a) = Q(s,a) - V(s)\n",
    "$$\n",
    "\n",
    "in the gradient ascent update. This [post](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/) nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "uj0_Tz5D4ZR-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107084,
     "status": "ok",
     "timestamp": 1710514211011,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "uj0_Tz5D4ZR-",
    "outputId": "4e615863-9a99-4770-e571-76b473fdff23"
   },
   "outputs": [],
   "source": [
    "#!pip install swig\n",
    "#!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205",
   "metadata": {
    "executionInfo": {
     "elapsed": 9191,
     "status": "ok",
     "timestamp": 1710514225212,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, device, distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch.distributions import Categorical\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ce09f3-e160-43b3-949a-736f79a10e62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1710514227660,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "f3ce09f3-e160-43b3-949a-736f79a10e62",
    "outputId": "e98d9040-c16b-44ef-9d8f-38854c96af7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  8\n",
      "Sample observation [-0.09705632  0.81044656  0.15335578  0.588963    1.1100553  -0.44759065\n",
      "  0.47545233  0.55648476]\n"
     ]
    }
   ],
   "source": [
    "env_id = \"LunarLander-v2\"\n",
    "env = gym.make(env_id)#,render_mode=\"human\")\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710514229946,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d"
   },
   "outputs": [],
   "source": [
    "device = device(\"cuda:0\" if cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877dfcb2-324c-402d-a75f-55a9decc8d5f",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710514231593,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "877dfcb2-324c-402d-a75f-55a9decc8d5f"
   },
   "outputs": [],
   "source": [
    "def calc_disc_return(r_t , gamma = 0.998):\n",
    "\n",
    "    G_t = deque(maxlen = len(r_t))\n",
    "    G_t.append(r_t[-1])\n",
    "\n",
    "    for i in reversed(r_t[:-1]):\n",
    "        disc = i + (gamma*G_t[0])\n",
    "        G_t.appendleft(disc)\n",
    "\n",
    "    return np.array(G_t)\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x,-1.1,1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return F.softmax(self.output(x),dim = 1)\n",
    "\n",
    "class ValueFunctionNet(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size):\n",
    "        super(ValueFunctionNet, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x,-1.1,1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zkop7So6oHv8",
   "metadata": {
    "id": "zkop7So6oHv8"
   },
   "source": [
    "The learning rate of the advantage network (also known as the value network) can vary depending on the specific implementation, architecture, and problem domain. However, typically it is chosen to be smaller than the learning rate of the policy network.\n",
    "\n",
    "The reason for this is that the advantage network is used to estimate the advantage or the baseline, which is then subtracted from the rewards to reduce variance. Since this network is indirectly influencing the updates to the policy network, having a lower learning rate can help stabilize training and prevent overshooting or divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96aabf3a-08fe-499f-aee8-b7699b50a619",
   "metadata": {
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1710514241598,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "96aabf3a-08fe-499f-aee8-b7699b50a619"
   },
   "outputs": [],
   "source": [
    "hidden_layer = 64\n",
    "gamma = 0.995\n",
    "policy_lr = 0.001\n",
    "value_lr = 0.001\n",
    "episodes = 100_000\n",
    "avg_win_size = 50\n",
    "epi_results = deque(maxlen=avg_win_size)\n",
    "\n",
    "policy_net = PolicyNet(s_size, a_size, hidden_layer).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr = policy_lr)\n",
    "\n",
    "value_net = ValueFunctionNet(s_size, hidden_layer).to(device)\n",
    "val_optimizer = optim.Adam(value_net.parameters(), lr = value_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8957793e-cec7-4aac-8b6d-fb808f290f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_name = os.path.join('.','artefacts',f'{env_id}_policygradient_bl.csv')\n",
    "log_file = open(log_file_name, \"w\")\n",
    "log_file.write(f'episode,loss,rewards,l2_grad,max_grad\\n')\n",
    "\n",
    "model_file = os.path.join('.','models',f'{env_id}_policygradient_bl.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcac59a-0b87-4ed7-9df2-3b4875cb062c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 840379,
     "status": "ok",
     "timestamp": 1710515084273,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "3fcac59a-0b87-4ed7-9df2-3b4875cb062c",
    "outputId": "e2be8c49-bada-4dee-e938-a155f088154e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi:05500 reward:   16.34 loss:  -91.91 mean_rewards:  184.38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epi in range(100_000):\n",
    "\n",
    "    s = env.reset()[0]\n",
    "    term , trunc = False, False\n",
    "    rewards, states , actions = [], [], []\n",
    "    win = 0\n",
    "\n",
    "    while not any([term, trunc]):\n",
    "\n",
    "        states.append(s)\n",
    "        obs = torch.FloatTensor(np.expand_dims(s,0)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p_vals = policy_net(obs)\n",
    "            p_vals = torch.squeeze(p_vals)\n",
    "\n",
    "        p_vals = p_vals.detach().cpu().numpy()\n",
    "        a = np.random.choice(a_size, p=p_vals)\n",
    "\n",
    "        s_, r, term ,trunc, _  = env.step(a)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s=np.copy(s_)\n",
    "\n",
    "    state_t = torch.FloatTensor(states).to(device)\n",
    "    action_t = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "    return_t = torch.FloatTensor(calc_disc_return(rewards, gamma)).to(device).view(-1,1)\n",
    "\n",
    "    vf_t = value_net(state_t).to(device)\n",
    "    with torch.no_grad():\n",
    "        advantage_t = return_t - vf_t\n",
    "\n",
    "    selected_action_prob = policy_net(state_t).gather(1, action_t)\n",
    "    loss = torch.mean(-torch.log(selected_action_prob) * advantage_t)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    vf_loss = loss_fn(vf_t, return_t)\n",
    "    val_optimizer.zero_grad()\n",
    "    vf_loss.backward()\n",
    "    val_optimizer.step()\n",
    "\n",
    "    grads = np.concatenate([p.grad.data.detach().cpu().numpy().flatten()\n",
    "    for p in policy_net.parameters()\n",
    "    if p.grad is not None])\n",
    "    \n",
    "    grad_l2 = np.sqrt(np.mean(np.square(grads)))\n",
    "    grad_max = np.max(np.abs(grads))\n",
    "    epi_results.append(np.sum(rewards))\n",
    "    log_file.write(f'{epi},{np.sum(rewards):.2f},{loss.item():.2f},{grad_l2:.4f},{grad_max:.4f}\\n')\n",
    "    \n",
    "    if epi%100==0:\n",
    "        clear_output()\n",
    "    if epi%10==0:\n",
    "        print(f'epi:{epi:05d} reward:{np.sum(rewards):8.2f} loss:{loss:8.2f} mean_rewards:{np.mean(epi_results):8.2f}')\n",
    "    if np.mean(np.mean(epi_results))>200:\n",
    "        break\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c50cc-054f-465e-8707-33f3581ca9f9",
   "metadata": {},
   "source": [
    "This technique to log grads is the book Deep Re-inforcement Learning.pdf chapter 12 page 313\n",
    "\n",
    "<a href = \"D:\\\\D Drive\\\\Docs\\\\Python\\\\Deep Re-inforcement Learning.pdf\"> ref. </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb1297a-c26c-4137-a559-414fc0f9f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, model_file)\n",
    "#model_scripted.save(model_file) # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c83c76c-5185-4456-b128-09dd3a2580fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = torch.load(model_file)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "_Hbm2_T2rRXM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101484,
     "status": "ok",
     "timestamp": 1710516736511,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "_Hbm2_T2rRXM",
    "outputId": "56c7bf3f-ab5b-486f-a938-49a466710605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi = 0 result 216.08\n",
      "epi = 1 result 170.79\n",
      "epi = 2 result 179.26\n",
      "epi = 3 result 157.73\n",
      "epi = 4 result 160.27\n",
      "epi = 5 result 171.25\n",
      "epi = 6 result 181.93\n",
      "epi = 7 result 239.37\n",
      "epi = 8 result 184.70\n",
      "epi = 9 result 235.15\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make(env_id,render_mode=\"human\")\n",
    "\n",
    "for epi in range(10):\n",
    "\n",
    "    s = eval_env.reset()[0]\n",
    "    term = False\n",
    "    trunc = False\n",
    "    score = 0\n",
    "    n=0\n",
    "    while not any([term, trunc]):\n",
    "\n",
    "        obs = torch.FloatTensor(np.expand_dims(s,0)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p_vals = saved_model(obs)\n",
    "            p_vals = torch.squeeze(p_vals)\n",
    "\n",
    "        p_vals = p_vals.detach().cpu().numpy()\n",
    "        #a = np.random.choice(a_size, p=p_vals)\n",
    "        a = np.argmax(p_vals)\n",
    "        s, r, term ,trunc , _  = eval_env.step(a)\n",
    "        #s = np.copy(s_)\n",
    "        #env.render()\n",
    "        n+=1\n",
    "        score+=r\n",
    "        if score >=200:\n",
    "            break\n",
    "\n",
    "    print(f'{epi = } result {score:4.2f}')\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb44e0-f720-4a36-aa14-409d6355a824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a93df-c64d-467d-9940-2bb2dbbad664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
