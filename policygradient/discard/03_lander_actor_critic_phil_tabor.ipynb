{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "FMNBAjTne9or",
   "metadata": {
    "id": "FMNBAjTne9or"
   },
   "source": [
    "[Hands on RL Policy Gradient](https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%204/4.3%20Policy%20Gradients%20REINFORCE.ipynb)\n",
    "\n",
    "[Policy Gradient Math](https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245)\n",
    "\n",
    "A widely used variation of REINFORCE is to subtract a baseline value from the return to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible). For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage:\n",
    "\n",
    "$$\n",
    "A(s,a) = Q(s,a) - V(s)\n",
    "$$\n",
    "\n",
    "in the gradient ascent update. This [post](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/) nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uj0_Tz5D4ZR-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107084,
     "status": "ok",
     "timestamp": 1710514211011,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "uj0_Tz5D4ZR-",
    "outputId": "4e615863-9a99-4770-e571-76b473fdff23"
   },
   "outputs": [],
   "source": [
    "#!pip install swig\n",
    "#!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac27d3-eb12-4cbf-90c2-e725a5eafb41",
   "metadata": {},
   "source": [
    "## Actor Critic\n",
    "\n",
    "![Reinforce_bl](acritic.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205",
   "metadata": {
    "executionInfo": {
     "elapsed": 9191,
     "status": "ok",
     "timestamp": 1710514225212,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205"
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import cuda, device, distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch.distributions import Categorical\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ce09f3-e160-43b3-949a-736f79a10e62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1710514227660,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "f3ce09f3-e160-43b3-949a-736f79a10e62",
    "outputId": "e98d9040-c16b-44ef-9d8f-38854c96af7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  4\n",
      "The Action Space is:  2\n",
      "Sample observation [-4.4889826e-01 -3.4762937e+37  1.8709894e-01  6.2282093e+37]\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)#,render_mode=\"human\")\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710514229946,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d"
   },
   "outputs": [],
   "source": [
    "device = device(\"cuda:0\" if cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96aabf3a-08fe-499f-aee8-b7699b50a619",
   "metadata": {
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1710514241598,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "96aabf3a-08fe-499f-aee8-b7699b50a619"
   },
   "outputs": [],
   "source": [
    "hidden_layer = 64\n",
    "gamma = 0.995\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.0001\n",
    "episodes = 100_000\n",
    "avg_win_size = 50\n",
    "epi_results = deque(maxlen=avg_win_size)\n",
    "\n",
    "#ac = ActorCritic(s_size, a_size, hidden_layer, actor_lr, critic_lr , gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10dbc8cc-8954-40f6-bb18-fbe460efb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                    dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                        dtype=np.float32)\n",
    "        self.log_probs = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
    "\n",
    "    def store_transition(self, state, log_prob, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.log_probs[index] = log_prob\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        probs = self.log_probs[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, probs, rewards, states_, terminal\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims,\n",
    "                 n_actions):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.pi = nn.Linear(self.fc2_dims, n_actions)\n",
    "        self.v = nn.Linear(self.fc2_dims, 1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        pi = self.pi(x)\n",
    "        v = self.v(x)\n",
    "        return (pi, v)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, lr, input_dims, n_actions, gamma=0.99,\n",
    "                 l1_size=32, l2_size=32, batch_size=32,\n",
    "                 mem_size=1000000):\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(mem_size, input_dims)\n",
    "        self.actor_critic = ActorCriticNetwork(lr, input_dims, l1_size,\n",
    "                                    l2_size, n_actions=n_actions)\n",
    "        self.log_probs = []\n",
    "\n",
    "    def store_transition(self, state, prob, reward, state_, done):\n",
    "        self.memory.store_transition(state, prob, reward, state_, done)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "       \n",
    "        state = T.tensor([observation]).to(self.actor_critic.device)\n",
    "        probabilities, _ = self.actor_critic.forward(state)\n",
    "        probabilities = F.softmax(probabilities)\n",
    "        action_probs = T.distributions.Categorical(probabilities)\n",
    "        action = action_probs.sample()\n",
    "        log_probs = action_probs.log_prob(action)\n",
    "\n",
    "        return action.item(), log_probs\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        self.actor_critic.optimizer.zero_grad()\n",
    "\n",
    "        state, prob, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(state).to(self.actor_critic.device)\n",
    "        probs = T.tensor(prob).to(self.actor_critic.device)\n",
    "        rewards = T.tensor(reward).to(self.actor_critic.device)\n",
    "        dones = T.tensor(done).to(self.actor_critic.device)\n",
    "        states_ = T.tensor(new_state).to(self.actor_critic.device)\n",
    "\n",
    "        _, critic_value_ = self.actor_critic.forward(states_)\n",
    "        _, critic_value = self.actor_critic.forward(states)\n",
    "\n",
    "        critic_value_[dones] = 0.0\n",
    "\n",
    "        delta = rewards + self.gamma*critic_value_\n",
    "\n",
    "        actor_loss = -T.mean(probs*(delta-critic_value))\n",
    "        critic_loss = F.mse_loss(delta, critic_value)\n",
    "\n",
    "        (actor_loss + critic_loss).backward()\n",
    "\n",
    "        self.actor_critic.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8da7b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "num_games = 1500    \n",
    "agent = Agent(lr=1e-5, input_dims=[s_size], n_actions=a_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94c5f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi:16000 reward:   17.00 avg_rewards:   17.51\n",
      "epi:16010 reward:   11.00 avg_rewards:   16.27\n",
      "epi:16020 reward:   28.00 avg_rewards:   16.96\n",
      "epi:16030 reward:   15.00 avg_rewards:   17.14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     15\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(observation, prob,\n\u001b[0;32m     16\u001b[0m                         reward, observation_, \u001b[38;5;28mint\u001b[39m(done))\n\u001b[1;32m---> 17\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation_\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m500\u001b[39m:\n",
      "Cell \u001b[1;32mIn[30], line 95\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     94\u001b[0m state, prob, reward, new_state, done \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 95\u001b[0m                         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m states \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mtensor(state)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     98\u001b[0m probs \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mtensor(prob)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[30], line 32\u001b[0m, in \u001b[0;36mReplayBuffer.sample_buffer\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     29\u001b[0m max_mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_cntr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_size)\n\u001b[0;32m     30\u001b[0m batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(max_mem, batch_size, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 32\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_memory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     33\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[batch]\n\u001b[0;32m     34\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_memory[batch]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "num_games = 100_000\n",
    "scores = []\n",
    "\n",
    "for i in range(num_games):\n",
    "    done = False\n",
    "    observation = env.reset()[0]\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        action, prob = agent.choose_action(observation)\n",
    "        observation_, reward, done, info , _ = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_transition(observation, prob,\n",
    "                                reward, observation_, int(done))\n",
    "        agent.learn()\n",
    "        observation = observation_\n",
    "        if score >500:\n",
    "            done = True\n",
    "        \n",
    "    scores.append(score)\n",
    "    avg_score = np.mean(scores[max(0, i-50):(i+1)])\n",
    "    '''\n",
    "    print('episode: ', i,'score %.1f ' % score,\n",
    "            ' average score %.1f' % avg_score)\n",
    "    '''\n",
    "\n",
    "    if i%100==0:\n",
    "        clear_output()\n",
    "    if i%10==0:\n",
    "        print(f'epi:{i:05d} reward:{score:8.2f} avg_rewards:{avg_score:8.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c88c037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04635795,  0.23186207,  0.02870866, -0.27556905], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ef288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
