{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "FMNBAjTne9or",
   "metadata": {
    "id": "FMNBAjTne9or"
   },
   "source": [
    "[Hands on RL Policy Gradient](https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%204/4.3%20Policy%20Gradients%20REINFORCE.ipynb)\n",
    "\n",
    "[Policy Gradient Math](https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245)\n",
    "\n",
    "A widely used variation of REINFORCE is to subtract a baseline value from the return to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible). For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage:\n",
    "\n",
    "$$\n",
    "A(s,a) = Q(s,a) - V(s)\n",
    "$$\n",
    "\n",
    "in the gradient ascent update. This [post](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/) nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uj0_Tz5D4ZR-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107084,
     "status": "ok",
     "timestamp": 1710514211011,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "uj0_Tz5D4ZR-",
    "outputId": "4e615863-9a99-4770-e571-76b473fdff23"
   },
   "outputs": [],
   "source": [
    "#!pip install swig\n",
    "#!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac27d3-eb12-4cbf-90c2-e725a5eafb41",
   "metadata": {},
   "source": [
    "## Actor Critic\n",
    "\n",
    "![Reinforce_bl](acritic.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205",
   "metadata": {
    "executionInfo": {
     "elapsed": 9191,
     "status": "ok",
     "timestamp": 1710514225212,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, device, distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch.distributions import Categorical\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3b1e34-38c2-4be0-8f8e-6865ab69e366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  8\n",
      "Sample observation [ 0.8586387  -0.41958687 -0.18232629 -2.5793898  -0.20207646 -3.4008355\n",
      "  0.39461595  0.91509247]\n"
     ]
    }
   ],
   "source": [
    "env_id = \"LunarLander-v2\"\n",
    "env = gym.make(env_id)#,render_mode=\"human\")\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710514229946,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d"
   },
   "outputs": [],
   "source": [
    "device = device(\"cuda:0\" if cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10dbc8cc-8954-40f6-bb18-fbe460efb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x,-1.1,1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return F.softmax(self.output(x),dim=-1) + 1e-8 #-1 to take softmax of last dimension\n",
    "    \n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x,-1.1,1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da7b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent():\n",
    "    def __init__(self, state_size, action_size, hidden_size, actor_lr, critic_lr, discount ):\n",
    "        self.action_size = action_size\n",
    "        self.actor_net = ActorNet(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic_net = CriticNet(state_size, hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_net.parameters(), lr=critic_lr)\n",
    "        self.discount = discount\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        #get action probs then randomly sample from the probabilities\n",
    "        with torch.no_grad():\n",
    "            input_state = torch.FloatTensor(state).to(device)\n",
    "            action_probs = self.actor_net(input_state)\n",
    "            #detach and turn to numpy to use with np.random.choice()\n",
    "            action_probs = action_probs.detach().cpu().numpy()\n",
    "            action = np.random.choice(np.arange(self.action_size), p=action_probs)\n",
    "        return action\n",
    "\n",
    "    def train(self, state_list, action_list, reward_list, next_state_list, done_list):\n",
    "        \n",
    "        # create tensors\n",
    "        state_t = torch.FloatTensor(state_list).to(device)\n",
    "        next_state_t = torch.FloatTensor(next_state_list).to(device)\n",
    "        action_t = torch.LongTensor(action_list).to(device).view(-1,1)\n",
    "        reward_t = torch.FloatTensor(reward_list).to(device).view(-1,1)\n",
    "        done_t = torch.FloatTensor(done_list).to(device).view(-1,1) #flipping 0 and 1 in RL training loop\n",
    "        \n",
    "        # get critic estimate\n",
    "        critic_t = self.critic_net(state_t).view(-1,1)\n",
    "        with torch.no_grad():\n",
    "            critic_td_t = reward_t + done_t * self.discount * self.critic_net(next_state_t).view(-1,1)\n",
    "            advantage_t = critic_td_t - critic_t\n",
    "        \n",
    "        # calculate actor loss\n",
    "        selected_action_prob = self.actor_net(state_t).gather(1, action_t)\n",
    "        actor_loss = torch.mean(-torch.log(selected_action_prob) * advantage_t)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step() \n",
    "\n",
    "        # calculate critic loss\n",
    "#         loss_fn = nn.MSELoss()\n",
    "#         critic_loss = loss_fn(critic_t, critic_td_t)\n",
    "        critic_loss = F.smooth_l1_loss(critic_t, critic_td_t)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step() \n",
    "        return actor_loss.detach().cpu().numpy(), critic_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31aafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = 64\n",
    "gamma = 0.995\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.001\n",
    "episodes = 100_000\n",
    "avg_win_size = 50\n",
    "epi_results = deque(maxlen=avg_win_size)\n",
    "\n",
    "agent = ActorCriticAgent(s_size, a_size, hidden_layer, actor_lr, critic_lr,  gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06be6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi:42700 reward:   38.14 timesteps:225\n",
      "epi:42710 reward:  275.75 timesteps:336\n"
     ]
    }
   ],
   "source": [
    "stats_rewards_list = [] # store stats for plotting in this\n",
    "stats_every = 10 # print stats every this many episodes\n",
    "total_reward = 0\n",
    "\n",
    "episode_length = 0\n",
    "stats_actor_loss, stats_critic_loss = [], []\n",
    "\n",
    "for epi in range(100_000):\n",
    "    done = False\n",
    "    state = env.reset()[0]\n",
    "    state_list, action_list, reward_list, next_state_list, done_list = [], [], [], [], []\n",
    "\n",
    "    time_steps = 0\n",
    "\n",
    "    # train in each episode until episode is done\n",
    "    while not done:\n",
    "       \n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        # enter action into the env\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        episode_length += 1\n",
    "        # end episode early\n",
    "        if total_reward < -250:\n",
    "            done = 1\n",
    "        # store agent's trajectory\n",
    "        state_list.append(state)\n",
    "        action_list.append(action)\n",
    "        reward_list.append(reward)\n",
    "        next_state_list.append(next_state)\n",
    "        done_list.append(1. - float(done))\n",
    "        \n",
    "        # train the agent\n",
    "        \n",
    "        state = next_state\n",
    "        time_steps+=1\n",
    "        \n",
    "    epi_results.append(np.sum(reward_list))\n",
    "    actor_loss, critic_loss = agent.train(state_list, action_list, reward_list, next_state_list, done_list)\n",
    "    total_reward = 0\n",
    "\n",
    "    if epi%100==0:\n",
    "        clear_output()\n",
    "    if epi%10==0:\n",
    "        print(f'epi:{epi:05d} reward:{np.sum(reward_list):8.2f} timesteps:{time_steps}')\n",
    "    if np.mean(np.mean(epi_results))>200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334a8c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
