{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "e0bf0078-ed5c-4003-ade7-da9b162f94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, device, distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gymnasium as gym\n",
    "import os, random\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "97f8b60a-59c7-4320-a645-795f9a659c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is 4\n",
      "The Action Space is 2\n",
      "Sample observation [-1.6941745e+00  3.1993532e+38  3.8102701e-01  2.3138998e+38]\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)#,render_mode=\"human\")\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(f'The State Space is {s_size}')\n",
    "print(f'The Action Space is {a_size}')\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "7bdeb303-1a37-4969-b8f9-cb5556ad31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device(\"cuda:0\" if cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "b8bd91b0-b396-4fd9-b8ef-5952d351f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor_layer = nn.Linear(state_size, hidden_size)\n",
    "        self.actor_output = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        self.critic_layer = nn.Linear(state_size, hidden_size)\n",
    "        self.critic_output = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        \n",
    "        obs = torch.clamp(obs,-1.1,1.1)\n",
    "        pi = F.relu(self.actor_layer(obs))\n",
    "        value = F.relu(self.critic_layer(obs))\n",
    "        \n",
    "        return F.softmax(self.actor_output(pi),dim = 1), self.critic_output(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "93c689a7-e341-413a-a182-3f3646b57f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = 16\n",
    "gamma = 0.99\n",
    "ac_lr = 0.001\n",
    "episodes = 100_000\n",
    "avg_win_size = 50\n",
    "epi_results = deque(maxlen=avg_win_size)\n",
    "\n",
    "agent = ActorCritic(s_size, a_size, hidden_layer).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr = ac_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e2dadcbb-03c3-41c9-ac3b-6873585ad747",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()[0]\n",
    "obs = torch.FloatTensor(np.expand_dims(s,0)).to(device)\n",
    "\n",
    "p_vals, values = agent(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "08b0e0d7-b6ee-44ca-a957-717a71b40ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03030152,  0.00010701,  0.01556824, -0.01421367], dtype=float32)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0bd9913f-44c0-4a3e-a72e-f883e30d7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epi in range(10):\n",
    "\n",
    "    s = env.reset()[0]\n",
    "    done , trunc = False, False\n",
    "    states, rewards, nxt_states , actions, dones  = [], [], [], [], []\n",
    "    win = 0\n",
    "\n",
    "    while not any([done, trunc]):\n",
    "\n",
    "        states.append(s)\n",
    "        obs = torch.FloatTensor(np.expand_dims(s,0)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p_vals, _ = agent(obs)\n",
    "            #value = value.detach().cpu().numpy()[0,0]\n",
    "            p_vals = torch.squeeze(p_vals)\n",
    "\n",
    "        p_vals = p_vals.detach().cpu().numpy()\n",
    "        a = np.random.choice(a_size, p=p_vals)\n",
    "\n",
    "        s_, r, term ,trunc, _  = env.step(a)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(int(done))\n",
    "        new_states.append(s_)\n",
    "        s = s_\n",
    "    \n",
    "    epi_results.append(np.sum(rewards))\n",
    "    \n",
    "    nxt_states_t = torch.FloatTensor(new_states).to(device)\n",
    "    states_t = torch.FloatTensor(states).to(device)\n",
    "    return_t = torch.FloatTensor(rewards).to(device).view(-1,1)\n",
    "    action_t = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "    \n",
    "    '''\n",
    "    p_vals , values = agent(states_t)\n",
    "    action_prob = p_vals.gather(1, action_t)\n",
    "\n",
    "    _, qval = agent(nxt_states_t)\n",
    "    qval = qval[-1].item()\n",
    "\n",
    "    #q_vals = np.zeros(values.shape[0])\n",
    "    q_vals = torch.zeros_like(values, device=device )\n",
    "\n",
    "    for t in reversed(range(values.shape[0])):\n",
    "\n",
    "        qval = rewards[t] + gamma * qval\n",
    "        q_vals[t] = qval\n",
    "\n",
    "    adv = q_vals - values\n",
    "    action_prob = p_vals.gather(1, action_t)\n",
    "\n",
    "    actor_loss = torch.mean(-torch.log(action_prob) * adv)\n",
    "    critic_loss = adv.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epi%100==0:\n",
    "        clear_output()\n",
    "    if epi%10==0:\n",
    "        print(f'epi:{epi:05d} reward:{np.sum(rewards):8.2f} mean_rewards:{np.mean(epi_results):8.2f}')\n",
    "    if np.mean(np.mean(epi_results))>=500:\n",
    "        break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "747aa436-6db3-41ea-9002-264ccb0d8913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "640bfd3a-6adf-4dfc-b378-58fa80bb0a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, qval = agent(torch.FloatTensor(np.expand_dims(s_,0)).to(device))\n",
    "q_val = qval.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "e65d5914-1b4f-49a4-a22c-b81f2f5f4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals , values = agent(states_t)\n",
    "action_prob = p_vals.gather(1, action_t)\n",
    "\n",
    "_, qval = agent(nxt_states_t)\n",
    "qval = qval[-1].item()\n",
    "\n",
    "#q_vals = np.zeros(values.shape[0])\n",
    "q_vals = torch.zeros_like(values, device=device )\n",
    "\n",
    "for t in reversed(range(values.shape[0])):\n",
    "    \n",
    "    qval = rewards[t] + gamma * qval\n",
    "    q_vals[t] = qval\n",
    "\n",
    "adv = q_vals - values\n",
    "action_prob = p_vals.gather(1, action_t)\n",
    "\n",
    "actor_loss = torch.mean(-torch.log(action_prob) * adv)\n",
    "critic_loss = adv.pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "5d8a52d2-c70d-4ce5-ad0d-307b065124b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(304427.0312, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = actor_loss + critic_loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "5699a142-b290-48f9-9ccd-8fd5f70f70ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[308], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m q_vals \u001b[38;5;241m=\u001b[39m \u001b[43mq_vals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "q_vals = q_vals.to(device).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "2787351d-40fd-4ea5-bd19-d3fcb3fecb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.zeros_like(values, dtype=torch.float64, device=device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f45ff0b1-c4f2-4b5e-b656-ebee38eaa390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "72aaf44e-7c94-4e58-97fe-f395b488f95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eeadfd71-f5ce-4654-a6d5-d6cd9ab08f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_states_t = torch.FloatTensor(new_states).to(device)\n",
    "states_t = torch.FloatTensor(states).to(device)\n",
    "return_t = torch.FloatTensor(rewards).to(device).view(-1,1)\n",
    "action_t = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "\n",
    "p_vals , values = agent(states_t)\n",
    "q_vals = return_t + gamma*agent(new_states_t)[1]\n",
    "adv = q_vals - values\n",
    "action_prob = p_vals.gather(1, action_t)\n",
    "\n",
    "actor_loss = torch.mean(-torch.log(action_prob) * adv)\n",
    "critic_loss = adv.pow(2).mean()\n",
    "\n",
    "loss = actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca4e9e7f-bd21-4b08-9b52-899e430c2089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cba60653-3489-4609-8e8a-b7b152a7e6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0140, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e04e9ccc-dd67-4175-b9b0-936d07a11ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5817],\n",
       "        [0.5641],\n",
       "        [0.8634],\n",
       "        [0.5633],\n",
       "        [0.5472],\n",
       "        [0.5158],\n",
       "        [0.9343],\n",
       "        [0.5121],\n",
       "        [0.4978],\n",
       "        [0.9407],\n",
       "        [0.4961],\n",
       "        [0.9419],\n",
       "        [0.4950]], device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(action_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1478d-6025-4a2b-88af-551f43134e10",
   "metadata": {},
   "source": [
    "[Chris Yoon](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)<br>\n",
    "[dilithjay](https://dilithjay.com/blog/actor-critic-methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "afb65eca-ffa0-4426-86f8-72e752881341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "num_steps = 500\n",
    "max_episodes = 100_000\n",
    "epi_results = deque(maxlen=avg_win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "5df566f3-0525-4601-b260-e4207270e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
    "\n",
    "        return value, policy_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9e406f11-f631-4309-9f50-2de4dcc709af",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_outputs = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "all_lengths = []\n",
    "average_lengths = []\n",
    "all_rewards = []\n",
    "entropy_term = 0\n",
    "\n",
    "for episode in range(10):\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "    for steps in range(num_steps):\n",
    "        value, policy_dist = actor_critic.forward(state)\n",
    "        value = value.detach().numpy()[0,0]\n",
    "        dist = policy_dist.detach().numpy() \n",
    "\n",
    "        action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "        log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "        entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        log_probs.append(log_prob)\n",
    "        entropy_term += entropy\n",
    "        state = new_state\n",
    "\n",
    "        if done or steps == num_steps-1:\n",
    "            Qval, _ = actor_critic.forward(new_state)\n",
    "            Qval = Qval.detach().numpy()[0,0]\n",
    "            all_rewards.append(np.sum(rewards))\n",
    "            all_lengths.append(steps)\n",
    "            average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "            if episode % 10 == 0:                    \n",
    "                sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "            if episode % 100 == 0:\n",
    "                clear_output()\n",
    "            break\n",
    "    \n",
    "    epi_results.append(np.sum(rewards))\n",
    "    '''\n",
    "    # compute Q values\n",
    "    Qvals = np.zeros_like(values)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        Qval = rewards[t] + GAMMA * Qval\n",
    "        Qvals[t] = Qval\n",
    "\n",
    "    #update actor critic\n",
    "    values = torch.FloatTensor(values)\n",
    "    Qvals = torch.FloatTensor(Qvals)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    \n",
    "    advantage = Qvals - values\n",
    "    actor_loss = (-log_probs * advantage).mean()\n",
    "    critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "    ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "    ac_optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    ac_optimizer.step()\n",
    "    \n",
    "    if np.mean(np.mean(epi_results))>=500:\n",
    "        break\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e1d322bc-e18b-48cc-b1c2-6e664e72a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qvals = np.zeros_like(values)\n",
    "for t in reversed(range(len(rewards))):\n",
    "    Qval = rewards[t] + GAMMA * Qval\n",
    "    Qvals[t] = Qval\n",
    "\n",
    "#update actor critic\n",
    "values = torch.FloatTensor(values)\n",
    "Qvals = torch.FloatTensor(Qvals)\n",
    "log_probs = torch.stack(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "c1ec243c-a26d-442b-834b-29b985cca3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage = Qvals - values\n",
    "actor_loss = (-log_probs * advantage).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "a0d59a18-644f-4e92-926e-cb138e2322bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss = 0.5 * advantage.pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "9df2ac33-0207-43f9-8060-554f65ea97c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9053, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "a3cb5279-aab6-470a-8ea0-159267a92888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(97.0633, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_loss + actor_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811926b-15e5-4c85-a730-a84d122b312a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
