{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "FMNBAjTne9or",
   "metadata": {
    "id": "FMNBAjTne9or"
   },
   "source": [
    "[Hands on RL Policy Gradient](https://github.com/PacktPublishing/Hands-on-Reinforcement-Learning-with-PyTorch/blob/master/Section%204/4.3%20Policy%20Gradients%20REINFORCE.ipynb)\n",
    "\n",
    "[Policy Gradient Math](https://towardsdatascience.com/policy-gradients-in-reinforcement-learning-explained-ecec7df94245)\n",
    "\n",
    "A widely used variation of REINFORCE is to subtract a baseline value from the return to reduce the variance of gradient estimation while keeping the bias unchanged (Remember we always want to do this when possible). For example, a common baseline is to subtract state-value from action-value, and if applied, we would use advantage:\n",
    "\n",
    "$$\n",
    "A(s,a) = Q(s,a) - V(s)\n",
    "$$\n",
    "\n",
    "in the gradient ascent update. This [post](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/) nicely explained why a baseline works for reducing the variance, in addition to a set of fundamentals of policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uj0_Tz5D4ZR-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107084,
     "status": "ok",
     "timestamp": 1710514211011,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "uj0_Tz5D4ZR-",
    "outputId": "4e615863-9a99-4770-e571-76b473fdff23"
   },
   "outputs": [],
   "source": [
    "#!pip install swig\n",
    "#!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac27d3-eb12-4cbf-90c2-e725a5eafb41",
   "metadata": {},
   "source": [
    "## Policy Gradient with Baseline\n",
    "\n",
    "![Reinforce_bl](acritic.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205",
   "metadata": {
    "executionInfo": {
     "elapsed": 9191,
     "status": "ok",
     "timestamp": 1710514225212,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "168d04e4-18b5-4a32-ac2c-d787c36bf205"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, device, distributions\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch.distributions import Categorical\n",
    "import math\n",
    "\n",
    "import gymnasium as gym\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ce09f3-e160-43b3-949a-736f79a10e62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1710514227660,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "f3ce09f3-e160-43b3-949a-736f79a10e62",
    "outputId": "e98d9040-c16b-44ef-9d8f-38854c96af7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "The State Space is:  8\n",
      "Sample observation [-0.9573402   0.90683043  2.0253897   0.03843589 -1.9880785  -1.2015078\n",
      "  0.12112498  0.19466022]\n"
     ]
    }
   ],
   "source": [
    "env_id = \"LunarLander-v2\"\n",
    "env = gym.make(env_id)#,render_mode=\"human\")\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n",
    "\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710514229946,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "92dc078a-f91b-40e3-9da3-97fb63177f3d"
   },
   "outputs": [],
   "source": [
    "device = device(\"cuda:0\" if cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877dfcb2-324c-402d-a75f-55a9decc8d5f",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710514231593,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "877dfcb2-324c-402d-a75f-55a9decc8d5f"
   },
   "outputs": [],
   "source": [
    "def calc_disc_return(r_t , gamma = 0.998):\n",
    "\n",
    "    G_t = deque(maxlen = len(r_t))\n",
    "    G_t.append(r_t[-1])\n",
    "\n",
    "    for i in reversed(r_t[:-1]):\n",
    "        disc = i + (gamma*G_t[0])\n",
    "        G_t.appendleft(disc)\n",
    "\n",
    "    return np.array(G_t)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x,-1.1,1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return F.softmax(self.output(x),dim = 1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x,-1.1,1.1)\n",
    "        x = F.relu(self.dense_layer_1(x))\n",
    "        x = F.relu(self.dense_layer_2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "class ActorCritic():\n",
    "    def __init__(self, state_size, action_size, hidden_size, gamma = 0.99, learning_rate = 0.001):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.actor = Actor(state_size, action_size, hidden_size).to(device)\n",
    "        self.critic = Critic(s_size, hidden_layer).to(device)\n",
    "        self.optimizer = optim.Adam(self.actor.parameters(), lr = learning_rate)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr = learning_rate)\n",
    "    \n",
    "    def train(self, states, rewards, actions):\n",
    "        \n",
    "        state_t = torch.FloatTensor(states).to(device)\n",
    "        action_t = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "        return_t = torch.FloatTensor(calc_disc_return(rewards, gamma)).to(device).view(-1,1)\n",
    "    \n",
    "        vf_t = self.value_net(state_t).to(device)\n",
    "        with torch.no_grad():\n",
    "            advantage_t = return_t - vf_t\n",
    "    \n",
    "        selected_action_prob = self.policy_net(state_t).gather(1, action_t)\n",
    "        loss = torch.mean(-torch.log(selected_action_prob) * advantage_t)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "        loss_fn = nn.MSELoss()\n",
    "        vf_loss = loss_fn(vf_t, return_t)\n",
    "        self.v_optimizer.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.v_optimizer.step()\n",
    "\n",
    "        grads = np.concatenate([p.grad.data.detach().cpu().numpy().flatten()\n",
    "                                for p in self.policy_net.parameters()\n",
    "                                if p.grad is not None])\n",
    "        \n",
    "        grad_l2 = np.sqrt(np.mean(np.square(grads)))\n",
    "        grad_max = np.max(np.abs(grads))\n",
    "    \n",
    "        return loss.item(), grad_l2, grad_max\n",
    "    \n",
    "    def save(self, model_file):\n",
    "        torch.save({\n",
    "            'actor_dict': self.actor.state_dict(),\n",
    "            'critic_dict': self.vcritic.state_dict(),\n",
    "            }, model_file)\n",
    "        \n",
    "    def load(self, model_file):\n",
    "        checkpoint = torch.load(model_file)\n",
    "        self.policy_net.load_state_dict(checkpoint['actor_dict'])\n",
    "        self.value_net.load_state_dict(checkpoint['critic_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96aabf3a-08fe-499f-aee8-b7699b50a619",
   "metadata": {
    "executionInfo": {
     "elapsed": 3269,
     "status": "ok",
     "timestamp": 1710514241598,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "96aabf3a-08fe-499f-aee8-b7699b50a619"
   },
   "outputs": [],
   "source": [
    "hidden_layer = 64\n",
    "gamma = 0.995\n",
    "policy_lr = 0.001\n",
    "value_lr = 0.001\n",
    "episodes = 3 #100_000\n",
    "avg_win_size = 50\n",
    "epi_results = deque(maxlen=avg_win_size)\n",
    "\n",
    "ac = ActorCritic(s_size, a_size, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8957793e-cec7-4aac-8b6d-fb808f290f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_name = os.path.join('.','artefacts',f'{env_id}_policygradient_ac.csv')\n",
    "model_file = os.path.join('.','models',f'{env_id}_policygradient_ac.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fcac59a-0b87-4ed7-9df2-3b4875cb062c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 840379,
     "status": "ok",
     "timestamp": 1710515084273,
     "user": {
      "displayName": "Arindam Dey",
      "userId": "10651625546197419632"
     },
     "user_tz": -330
    },
    "id": "3fcac59a-0b87-4ed7-9df2-3b4875cb062c",
    "outputId": "e2be8c49-bada-4dee-e938-a155f088154e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = True\n",
    "'''\n",
    "if train:\n",
    "    log_file = open(log_file_name, \"w\")\n",
    "    log_file.write(f'episode,rewards,loss,l2_grad,max_grad\\n')\n",
    "'''\n",
    "\n",
    "for epi in range(episodes):\n",
    "\n",
    "    if not train:\n",
    "        print(\"set train flag to True for Training\")\n",
    "        break\n",
    "        \n",
    "    s = env.reset()[0]\n",
    "    done , trunc = False, False\n",
    "    states , nxt_states, rewards, actions, dones = [], [], [], [], []\n",
    "    win = 0\n",
    "\n",
    "    while not any([done, trunc]):\n",
    "\n",
    "        states.append(s)\n",
    "        obs = torch.FloatTensor(np.expand_dims(s,0)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            p_vals = ac.actor(obs)\n",
    "            p_vals = torch.squeeze(p_vals)\n",
    "\n",
    "        p_vals = p_vals.detach().cpu().numpy()\n",
    "        a = np.random.choice(a_size, p=p_vals)\n",
    "\n",
    "        s_, r, done ,trunc, _  = env.step(a)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        dones.append(done)\n",
    "        nxt_states.append(s_)\n",
    "        \n",
    "        s=np.copy(s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cbe0985-a06b-47a3-9d5d-989fc9664a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arind\\AppData\\Local\\Temp\\ipykernel_18024\\1488688309.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  state_t = torch.FloatTensor(states).to(device)\n"
     ]
    }
   ],
   "source": [
    "state_t = torch.FloatTensor(states).to(device)\n",
    "action_t = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "return_t = torch.FloatTensor(calc_disc_return(rewards, gamma)).to(device).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afad21d0-c69e-44de-a344-41f5f5768c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prob = ac.actor(state_t).gather(1, action_t)\n",
    "#loss = torch.mean(-torch.log(action_prob) * return_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b39f8329-d2f2-45bc-a957-ee719e54fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = len(rewards) \n",
    "discounts = np.logspace(0, T, num=T, base=gamma, endpoint=False)\n",
    "\n",
    "returns = np.array([np.sum(discounts[:T-t] * rewards[t:]) for t in range(T)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bfa0d6d-1df3-49ec-9ab4-1b9678fd9696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-261.2838],\n",
       "        [-261.1110],\n",
       "        [-260.6499],\n",
       "        [-259.7742],\n",
       "        [-258.5236],\n",
       "        [-259.9392],\n",
       "        [-261.2383],\n",
       "        [-262.8831],\n",
       "        [-264.5254],\n",
       "        [-266.8300],\n",
       "        [-265.9666],\n",
       "        [-266.0929],\n",
       "        [-265.3258],\n",
       "        [-263.9409],\n",
       "        [-265.4377],\n",
       "        [-268.0513],\n",
       "        [-266.6037],\n",
       "        [-265.9030],\n",
       "        [-265.1532],\n",
       "        [-265.4020],\n",
       "        [-265.9242],\n",
       "        [-269.3294],\n",
       "        [-268.9385],\n",
       "        [-267.6998],\n",
       "        [-266.1357],\n",
       "        [-268.3803],\n",
       "        [-271.0439],\n",
       "        [-271.5903],\n",
       "        [-270.0543],\n",
       "        [-270.2497],\n",
       "        [-268.6083],\n",
       "        [-267.7786],\n",
       "        [-268.0731],\n",
       "        [-266.5521],\n",
       "        [-264.8605],\n",
       "        [-264.7093],\n",
       "        [-264.5811],\n",
       "        [-265.6229],\n",
       "        [-266.0389],\n",
       "        [-265.0164],\n",
       "        [-263.7697],\n",
       "        [-262.3778],\n",
       "        [-261.6683],\n",
       "        [-259.8918],\n",
       "        [-258.2677],\n",
       "        [-257.9777],\n",
       "        [-256.3021],\n",
       "        [-257.8589],\n",
       "        [-257.4756],\n",
       "        [-260.2858],\n",
       "        [-258.4280],\n",
       "        [-256.2694],\n",
       "        [-257.4469],\n",
       "        [-257.5717],\n",
       "        [-255.4210],\n",
       "        [-255.6762],\n",
       "        [-254.9817],\n",
       "        [-253.3949],\n",
       "        [-251.8202],\n",
       "        [-251.7206],\n",
       "        [-249.2204],\n",
       "        [-247.8167],\n",
       "        [-247.1111],\n",
       "        [-245.5026],\n",
       "        [-245.0027],\n",
       "        [-243.6102],\n",
       "        [-240.7993],\n",
       "        [-240.2554],\n",
       "        [-239.0473],\n",
       "        [-236.8014],\n",
       "        [-235.3539],\n",
       "        [-233.1285],\n",
       "        [-229.6814],\n",
       "        [-228.2008],\n",
       "        [-224.2801],\n",
       "        [-218.4637],\n",
       "        [-215.6793],\n",
       "        [-208.8597],\n",
       "        [-206.9439],\n",
       "        [-203.9367],\n",
       "        [-201.5898],\n",
       "        [-197.0878],\n",
       "        [-193.5193],\n",
       "        [-187.5292],\n",
       "        [-179.4490],\n",
       "        [-175.2392],\n",
       "        [-168.7305],\n",
       "        [-169.1947],\n",
       "        [-163.6268],\n",
       "        [-156.6648],\n",
       "        [-146.1729],\n",
       "        [-124.9748],\n",
       "        [-100.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efebee83-9bb5-48fe-a65a-bbba4762bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "## eps is the smallest representable float, which is\n",
    "# added to the standard deviation of the returns to avoid numerical instabilities\n",
    "#returns = torch.tensor(returns)\n",
    "return_t = (return_t - return_t.mean()) / (return_t.std() + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c25aa06-48d3-4286-888d-971a9d18629d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5166],\n",
       "        [-0.5118],\n",
       "        [-0.4989],\n",
       "        [-0.4745],\n",
       "        [-0.4396],\n",
       "        [-0.4791],\n",
       "        [-0.5154],\n",
       "        [-0.5613],\n",
       "        [-0.6071],\n",
       "        [-0.6714],\n",
       "        [-0.6473],\n",
       "        [-0.6509],\n",
       "        [-0.6294],\n",
       "        [-0.5908],\n",
       "        [-0.6326],\n",
       "        [-0.7055],\n",
       "        [-0.6651],\n",
       "        [-0.6456],\n",
       "        [-0.6246],\n",
       "        [-0.6316],\n",
       "        [-0.6461],\n",
       "        [-0.7412],\n",
       "        [-0.7303],\n",
       "        [-0.6957],\n",
       "        [-0.6521],\n",
       "        [-0.7147],\n",
       "        [-0.7890],\n",
       "        [-0.8043],\n",
       "        [-0.7614],\n",
       "        [-0.7669],\n",
       "        [-0.7211],\n",
       "        [-0.6979],\n",
       "        [-0.7061],\n",
       "        [-0.6637],\n",
       "        [-0.6165],\n",
       "        [-0.6122],\n",
       "        [-0.6087],\n",
       "        [-0.6377],\n",
       "        [-0.6494],\n",
       "        [-0.6208],\n",
       "        [-0.5860],\n",
       "        [-0.5472],\n",
       "        [-0.5274],\n",
       "        [-0.4778],\n",
       "        [-0.4325],\n",
       "        [-0.4244],\n",
       "        [-0.3776],\n",
       "        [-0.4210],\n",
       "        [-0.4103],\n",
       "        [-0.4888],\n",
       "        [-0.4369],\n",
       "        [-0.3767],\n",
       "        [-0.4095],\n",
       "        [-0.4130],\n",
       "        [-0.3530],\n",
       "        [-0.3601],\n",
       "        [-0.3407],\n",
       "        [-0.2965],\n",
       "        [-0.2525],\n",
       "        [-0.2497],\n",
       "        [-0.1799],\n",
       "        [-0.1408],\n",
       "        [-0.1211],\n",
       "        [-0.0762],\n",
       "        [-0.0622],\n",
       "        [-0.0234],\n",
       "        [ 0.0551],\n",
       "        [ 0.0703],\n",
       "        [ 0.1040],\n",
       "        [ 0.1667],\n",
       "        [ 0.2071],\n",
       "        [ 0.2692],\n",
       "        [ 0.3654],\n",
       "        [ 0.4067],\n",
       "        [ 0.5161],\n",
       "        [ 0.6785],\n",
       "        [ 0.7562],\n",
       "        [ 0.9465],\n",
       "        [ 1.0000],\n",
       "        [ 1.0839],\n",
       "        [ 1.1494],\n",
       "        [ 1.2751],\n",
       "        [ 1.3747],\n",
       "        [ 1.5419],\n",
       "        [ 1.7674],\n",
       "        [ 1.8849],\n",
       "        [ 2.0665],\n",
       "        [ 2.0536],\n",
       "        [ 2.2090],\n",
       "        [ 2.4033],\n",
       "        [ 2.6961],\n",
       "        [ 3.2878],\n",
       "        [ 3.9848]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb30b31c-8821-405d-938d-2eac43688e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = torch.FloatTensor(states).to(device)\n",
    "action_t = torch.LongTensor(actions).to(device).view(-1,1)\n",
    "return_t = torch.FloatTensor(calc_disc_return(rewards, gamma)).to(device).view(-1,1)\n",
    "nxt_state_t = torch.FloatTensor(nxt_states).to(device)\n",
    "done_t = torch.FloatTensor(dones).to(device).view(-1,1)\n",
    "\n",
    "critic_t = ac.critic(state_t).view(-1,1)\n",
    "with torch.no_grad():\n",
    "    critic_td_t = return_t + done_t * ac.gamma * ac.critic(nxt_state_t).view(-1,1)\n",
    "    advantage_t = critic_td_t - critic_t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6bf1da3-18c0-438d-8f9b-196b779d86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_action_prob = ac.actor(state_t).gather(1, action_t)\n",
    "actor_loss = torch.mean(-torch.log(selected_action_prob) * advantage_t)\n",
    "ac.optimizer.zero_grad()\n",
    "actor_loss.backward()\n",
    "ac.optimizer.step() \n",
    "\n",
    "\n",
    "critic_loss = F.smooth_l1_loss(critic_t, critic_td_t)\n",
    "ac.critic_optimizer.zero_grad()\n",
    "critic_loss.backward()\n",
    "ac.critic_optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6e0b3b8-9989-49ba-a6ec-2128e9f7a608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(128.5814, device='cuda:0', grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbc8cc-8954-40f6-bb18-fbe460efb580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
